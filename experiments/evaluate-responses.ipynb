{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from llama_index import VectorStoreIndex, SimpleWebPageReader, ServiceContext\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "from llama_index.evaluation import FaithfulnessEvaluator, \\\n",
    "    RelevancyEvaluator\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.prompts import PromptTemplate\n",
    "from llama_index.query_engine import CustomQueryEngine\n",
    "from llama_index.response.schema import Response\n",
    "from llama_index.response_synthesizers import (\n",
    "    get_response_synthesizer,\n",
    "    BaseSynthesizer,\n",
    ")\n",
    "from llama_index.retrievers import VectorIndexRetriever, BaseRetriever\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<Put the OpenAI API Key here>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, model_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents(url: str):\n",
    "    return SimpleWebPageReader(html_to_text=True).load_data([url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding():\n",
    "    model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "    model_kwargs = {'device': 'cpu'}\n",
    "    encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "    hf = HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    return hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = get_documents(\"https://www.pg.unicamp.br/norma/31594/0\")\n",
    "embedding_model = get_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our RAG pipeline uses gpt-3.5-turbo to index and query our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_parser = SimpleNodeParser.from_defaults(\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=500)\n",
    ")\n",
    "\n",
    "gpt35_llm = OpenAI(temperature=0.1, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "gpt35_service_context = ServiceContext.from_defaults(\n",
    "    llm=gpt35_llm,\n",
    "    embed_model=OpenAIEmbedding(),\n",
    "    node_parser=node_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring a Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4100a84d19654747a408ca6122f9821f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing documents into nodes:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cafda848795481b8d50cd6c1935517c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# build index\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=gpt35_service_context, show_progress=True)\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=4,\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=\"compact\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_qa_template = PromptTemplate(\n",
    "    \"As informações dos documentos estão apresentadas abaixo.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Dadas as informações dos documentos e nenhum conhecimento prévio, \"\n",
    "    \"responda a seguinte pergunta.\\n\"\n",
    "    \"Pergunta: {query_str}\\n\"\n",
    "    \"Resposta: \"\n",
    ")\n",
    "\n",
    "refine_template = PromptTemplate(\n",
    "    \"A pergunta original é a seguinte: {query_str}\\n\"\n",
    "    \"Fornecemos uma resposta existente: {existing_answer}\\n\"\n",
    "    \"Temos a oportunidade de refinar a resposta existente (somente se necessário) com mais contexto abaixo.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{context_msg}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"Dado o novo contexto, refine a resposta original para melhor responder à pergunta. \"\n",
    "    \"Se o contexto não for útil, retorne APENAS a resposta original.\\n\"\n",
    "\n",
    "    \"Resposta: \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_synthesizer.update_prompts({\"text_qa_template\": text_qa_template, \"refine_template\": refine_template})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our pipeline will evaluate both the response quality and hallucination. In the evaluation process e will setup `gpt-3.5-turbo` as our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T23:43:57.292923442Z",
     "start_time": "2023-11-07T23:43:57.269794766Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "qa_pairs = pickle.load(open(\"../eval_data/eval_dataset.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T23:43:58.786367729Z",
     "start_time": "2023-11-07T23:43:58.763660110Z"
    }
   },
   "outputs": [],
   "source": [
    "questions, answers = zip(*qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questions</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Qual é o total de vagas oferecidas para o Vest...</td>\n",
       "      <td>Para o ano de 2024, a Universidade Estadual de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quais são as condições para que um candidato p...</td>\n",
       "      <td>Os candidatos que podem participar do Programa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Quais são as datas e o formato das provas para...</td>\n",
       "      <td>A 1ª fase do VU 2024 será realizada no dia 29 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Qual é a consequência para um candidato que ob...</td>\n",
       "      <td>O candidato que não realizar ou obtiver nota 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Qual é o procedimento para a classificação e c...</td>\n",
       "      <td>Para cada curso, até duas provas são considera...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           questions  \\\n",
       "0  Qual é o total de vagas oferecidas para o Vest...   \n",
       "1  Quais são as condições para que um candidato p...   \n",
       "2  Quais são as datas e o formato das provas para...   \n",
       "3  Qual é a consequência para um candidato que ob...   \n",
       "4  Qual é o procedimento para a classificação e c...   \n",
       "\n",
       "                                             answers  \n",
       "0  Para o ano de 2024, a Universidade Estadual de...  \n",
       "1  Os candidatos que podem participar do Programa...  \n",
       "2  A 1ª fase do VU 2024 será realizada no dia 29 ...  \n",
       "3  O candidato que não realizar ou obtiver nota 0...  \n",
       "4  Para cada curso, até duas provas são considera...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_df = pd.DataFrame.from_dict({\n",
    "    \"questions\": questions,\n",
    "    \"answers\": answers\n",
    "})\n",
    "\n",
    "print(qa_df.shape)\n",
    "qa_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnicampQueryEngine(CustomQueryEngine):\n",
    "    \"\"\"RAG String Query Engine.\"\"\"\n",
    "\n",
    "    retriever: BaseRetriever\n",
    "    response_synthesizer: BaseSynthesizer\n",
    "\n",
    "    def custom_query(self, query_str: str):\n",
    "        nodes = self.retriever.retrieve(query_str)\n",
    "        response_obj = self.response_synthesizer.synthesize(query_str, nodes)\n",
    "\n",
    "        return response_obj\n",
    "\n",
    "    async def acustom_query(self, query_str: str):\n",
    "        \"\"\"Run a custom query asynchronously.\"\"\"\n",
    "        # by default, just run the synchronous version\n",
    "        return self.custom_query(query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = UnicampQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maria Luiza Moretti é a Reitora em exercício da Unicamp.\n"
     ]
    }
   ],
   "source": [
    "query_str = \"Quem é Maria Luiza Moretti?\"\n",
    "\n",
    "response = query_engine.custom_query(query_str)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "async def run_query(query_engine, q):\n",
    "    try:\n",
    "        return await query_engine.acustom_query(q)\n",
    "    except:\n",
    "        return Response(response=\"Error, query failed.\")\n",
    "\n",
    "\n",
    "def async_evaluate_query_engine(evaluator, query_engine, questions, batch_size=10):\n",
    "    total_correct = 0\n",
    "    all_results = []\n",
    "    for i in tqdm(range(0, len(questions), batch_size)):\n",
    "        batch_qs = questions[i:i + batch_size]\n",
    "\n",
    "        tasks = [run_query(query_engine, q) for q in batch_qs]\n",
    "        responses = asyncio.run(asyncio.gather(*tasks))\n",
    "        print(f\"finished batch {(i // batch_size) + 1} out of {len(questions) // batch_size + 1}\")\n",
    "\n",
    "        # eval for hallucination\n",
    "        if isinstance(evaluator, FaithfulnessEvaluator):\n",
    "            print(\"Use FaithfulnessEvaluator\")\n",
    "            for response in responses:\n",
    "                eval_result = 1 if \"YES\" in evaluator.evaluate_response(response=response).feedback else 0\n",
    "                total_correct += eval_result\n",
    "                all_results.append(eval_result)\n",
    "\n",
    "        # eval for answer quality\n",
    "        elif isinstance(evaluator, RelevancyEvaluator):\n",
    "            print(\"Use RelevancyEvaluator\")\n",
    "            for question, response in zip(batch_qs, responses):\n",
    "                context_list = response.source_nodes\n",
    "                eval_result = 1 if \"YES\" in evaluator.evaluate_response(query=question, response=response,\n",
    "                                                                        context=context_list).feedback else 0\n",
    "                total_correct += eval_result\n",
    "                all_results.append(eval_result)\n",
    "\n",
    "        # helps avoid rate limits\n",
    "        time.sleep(1)\n",
    "\n",
    "    return total_correct, all_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e8eaa90e0c49d093a74ccaa7727a94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished batch 1 out of 2\n",
      "Use FaithfulnessEvaluator\n",
      "finished batch 2 out of 2\n",
      "Use FaithfulnessEvaluator\n"
     ]
    }
   ],
   "source": [
    "# eval for Faithfulness/hallucination\n",
    "faithfulness_evaluator = FaithfulnessEvaluator(service_context=gpt35_service_context)\n",
    "total_correct, all_results = async_evaluate_query_engine(faithfulness_evaluator, query_engine, questions, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness:  Scored 22 out of 24 questions correctly.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Faithfulness:  Scored {total_correct} out of {len(questions)} questions correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3b412df961549afb9b34e9f97b14944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished batch 1 out of 2\n",
      "Use RelevancyEvaluator\n",
      "finished batch 2 out of 2\n",
      "Use RelevancyEvaluator\n"
     ]
    }
   ],
   "source": [
    "# eval for Relevancy/answer quality\n",
    "relevancy_evaluator = RelevancyEvaluator(service_context=gpt35_service_context)\n",
    "total_correct, all_results = async_evaluate_query_engine(relevancy_evaluator, query_engine, questions, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevancy:  Scored 21 out of 24 questions correctly.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Relevancy:  Scored {total_correct} out of {len(questions)} questions correctly.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
